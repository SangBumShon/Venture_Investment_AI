# %%
# !pip install -r requirements.txt

# %%
from dotenv import load_dotenv

load_dotenv()

# %%
# from langchain_teddynote import logging
# logging.langsmith("AI-project")

# %%
from typing import TypedDict, Literal, List, Dict, Any
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain_core.prompts import ChatPromptTemplate
from utils import (
    search_tavily, search_naver_news, get_analysis_score, 
    extract_insufficient_items, format_checklist_items, create_analysis_prompt_template
)

from reportlab.lib.pagesizes import letter
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle
from reportlab.lib import colors
import markdown2
import pdfkit
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
import datetime
import os
import re
import requests

# %%
# 1. ìƒíƒœ ìŠ¤í‚¤ë§ˆ ì •ì˜ (ëª¨ë“  Agentê°€ ê³µìœ )
class AgentState(TypedDict, total=False):
    startup_name: str  # ìŠ¤íƒ€íŠ¸ì—… ì´ë¦„ ì¶”ê°€
    ìƒí’ˆ_ì ìˆ˜: int
    ê¸°ìˆ _ì ìˆ˜: int
    ì„±ì¥ë¥ _ì ìˆ˜: int
    ì‹œì¥ì„±_ì ìˆ˜: int
    ê²½ìŸì‚¬_ì ìˆ˜: int
    ìµœì¢…_íŒë‹¨: Literal["íˆ¬ì", "ë³´ë¥˜"]
    ë³´ê³ ì„œ: str
    pdf_path: str  # PDF íŒŒì¼ ê²½ë¡œ ì¶”ê°€
    # ë¶„ì„ ê·¼ê±° ì¶”ê°€
    ìƒí’ˆ_ë¶„ì„_ê·¼ê±°: str
    ê¸°ìˆ _ë¶„ì„_ê·¼ê±°: str
    ì„±ì¥ë¥ _ë¶„ì„_ê·¼ê±°: str
    ì‹œì¥ì„±_ë¶„ì„_ê·¼ê±°: str
    ê²½ìŸì‚¬_ë¶„ì„_ê·¼ê±°: str

# %%
# 2. LLM ì´ˆê¸°í™”
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# %%

def analyze_product(state: AgentState) -> AgentState:
    startup_name = state.get("startup_name", "")
    if not startup_name:
        state["ìƒí’ˆ_ì ìˆ˜"] = 0
        state["ìƒí’ˆ_ë¶„ì„_ê·¼ê±°"] = "ìŠ¤íƒ€íŠ¸ì—… ì´ë¦„ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        return state

    try:
        # API í‚¤ ê²€ì¦
        from utils import validate_api_keys
        validate_api_keys(["tavily_key", "naver_id", "naver_secret"])
    except ValueError as e:
        state["ìƒí’ˆ_ì ìˆ˜"] = 0
        state["ìƒí’ˆ_ë¶„ì„_ê·¼ê±°"] = str(e)
        return state

    checklist = [
        "ì œí’ˆì´ ëª…í™•í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ”ê°€?",
        "ì œí’ˆ ê¸°ëŠ¥ì´ ì‚¬ìš©ìê°€ ê¸°ëŒ€í•˜ëŠ” ê°€ì¹˜ë¥¼ ì œê³µí•˜ëŠ”ê°€?",
        "ì œí’ˆì˜ ì°¨ë³„í™” ìš”ì†Œê°€ ëª…í™•í•œê°€?",
        "ì œí’ˆì˜ ì‚¬ìš©ì„±(UI/UX)ì´ ì§ê´€ì ì¸ê°€?",
        "ì œí’ˆì˜ ê¸°ìˆ ì  êµ¬í˜„ ê°€ëŠ¥ì„±ì´ ë†’ì€ê°€?",
        "ì œí’ˆì˜ ì‹œì¥ ìˆ˜ìš”ê°€ ì¶©ë¶„í•œê°€?",
        "ì œí’ˆ ê°€ê²© ì „ëµì´ í•©ë¦¬ì ì¸ê°€?",
        "ì œí’ˆ ì¶œì‹œ ë° í™•ì¥ ê³„íšì´ êµ¬ì²´ì ì¸ê°€?",
        "ê²½ìŸ ì œí’ˆ ëŒ€ë¹„ ìš°ìœ„ê°€ ìˆëŠ”ê°€?",
        "ê³ ê° í”¼ë“œë°± ìˆ˜ì§‘ ë° ë°˜ì˜ ì²´ê³„ê°€ ê°–ì¶°ì ¸ ìˆëŠ”ê°€?"
    ]
    items_formatted = format_checklist_items(checklist)

    # Tavily & Naver ê²€ìƒ‰
    tavily_context = search_tavily(startup_name, 5)
    naver_context = search_naver_news(startup_name, 5)

    # LLM Prompt
    prompt_template = create_analysis_prompt_template(checklist)
    prompt = ChatPromptTemplate.from_template(prompt_template)
    response = (prompt | llm).invoke({
        "startup_name": startup_name,
        "items": items_formatted,
        "tavily_context": tavily_context,
        "naver_context": naver_context
    })

    # ê²°ê³¼ íŒŒì‹±
    analysis = response.content
    score = get_analysis_score(analysis, checklist)

    state["ìƒí’ˆ_ì ìˆ˜"] = score
    state["ìƒí’ˆ_ë¶„ì„_ê·¼ê±°"] = analysis
    print(state["ìƒí’ˆ_ì ìˆ˜"])
    print(state["ìƒí’ˆ_ë¶„ì„_ê·¼ê±°"])
    return state



# ë©”ì¸ í•¨ìˆ˜
def analyze_technology(state: "AgentState") -> "AgentState":
    startup_name = state.get("startup_name", "")
    if not startup_name:
        state["ê¸°ìˆ _ì ìˆ˜"] = 0
        state["ê¸°ìˆ _ë¶„ì„_ê·¼ê±°"] = "ìŠ¤íƒ€íŠ¸ì—… ì´ë¦„ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        return state

    try:
        # API í‚¤ ê²€ì¦
        from utils import validate_api_keys
        validate_api_keys(["tavily_key", "naver_id", "naver_secret"])
    except ValueError as e:
        state["ê¸°ìˆ _ì ìˆ˜"] = 0
        state["ê¸°ìˆ _ë¶„ì„_ê·¼ê±°"] = str(e)
        return state

    checklist = [
        "ê¸°ìˆ ì  ì°¨ë³„ì„±", "íŠ¹í—ˆ ë³´ìœ  ì—¬ë¶€", "ìŠ¤ì¼€ì¼ë§ ê°€ëŠ¥ì„±", "ê¸°ìˆ  ì„±ìˆ™ë„",
        "ì¸ë ¥ ì—­ëŸ‰", "ê¸°ìˆ  ë‚œì´ë„", "ê¸°ìˆ  êµ¬í˜„ ê°€ëŠ¥ì„±", "ê¸°ìˆ  ìœ ì§€ë³´ìˆ˜ ìš©ì´ì„±",
        "ê¸°ìˆ  í‘œì¤€ ì¤€ìˆ˜ ì—¬ë¶€", "ê¸°ìˆ  ê´€ë ¨ ì™¸ë¶€ ì¸ì¦ ë˜ëŠ” ìˆ˜ìƒ ì´ë ¥"
    ]
    items_formatted = format_checklist_items(checklist)

    prompt_template = create_analysis_prompt_template(checklist)
    prompt = ChatPromptTemplate.from_template(prompt_template)

    analysis = ""
    for attempt in range(1, 4):
        # Tavily & Naver ê²€ìƒ‰
        tavily_context = search_tavily(startup_name, 5)

        # Naver ê²€ìƒ‰
        naver_context = search_naver_news(startup_name, 5)

        # LLM í˜¸ì¶œ
        response = (prompt | llm).invoke({
            "startup_name": startup_name,
            "items": items_formatted,
            "tavily_context": tavily_context,
            "naver_context": naver_context
        })
        analysis = response.content

        insufficient = extract_insufficient_items(analysis, checklist)
        if len(insufficient) < 5 or attempt == 3:
            break

    # robust ì ìˆ˜ íŒŒì‹±
    score = get_analysis_score(analysis, checklist)

    state["ê¸°ìˆ _ì ìˆ˜"] = score
    state["ê¸°ìˆ _ë¶„ì„_ê·¼ê±°"] = analysis
    return state



def analyze_growth(state: "AgentState") -> "AgentState":
    startup_name = state.get("startup_name", "")
    if not startup_name:
        state["ì„±ì¥ë¥ _ì ìˆ˜"] = 0
        state["ì„±ì¥ë¥ _ë¶„ì„_ê·¼ê±°"] = "ìŠ¤íƒ€íŠ¸ì—… ì´ë¦„ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        return state

    try:
        # API í‚¤ ê²€ì¦
        from utils import validate_api_keys
        validate_api_keys(["tavily_key", "naver_id", "naver_secret"])
    except ValueError as e:
        state["ì„±ì¥ë¥ _ì ìˆ˜"] = 0
        state["ì„±ì¥ë¥ _ë¶„ì„_ê·¼ê±°"] = str(e)
        return state

    checklist = [
        "ë§¤ì¶œ ì„±ì¥ë¥ ", "ì‚¬ìš©ì ì¦ê°€ìœ¨", "ì‹œì¥ ì ìœ ìœ¨ ë³€í™”", "ê³ ê° ìœ ì§€ìœ¨ (Retention Rate)",
        "ì›”ê°„/ë¶„ê¸°ë³„ í™œì„± ì‚¬ìš©ì ì¦ê°€ (MAU/WAU)", "ì‹ ê·œ ê³„ì•½/í´ë¼ì´ì–¸íŠ¸ ìˆ˜ ì¦ê°€", "ì—°ê°„ ë°˜ë³µ ë§¤ì¶œ(ARR) ì„±ì¥",
        "íˆ¬ì ìœ ì¹˜ ê·œëª¨ ë³€í™”", "ì§ì› ìˆ˜ ì¦ê°€ìœ¨", "í•´ì™¸/ì‹ ì‹œì¥ ì§„ì¶œ ì†ë„"
    ]
    items_formatted = format_checklist_items(checklist)

    prompt_template = create_analysis_prompt_template(checklist)
    prompt = ChatPromptTemplate.from_template(prompt_template)

    # Tavily & Naver ê²€ìƒ‰
    tavily_context = search_tavily(startup_name, 5)
    naver_context = search_naver_news(startup_name, 5)

    response = (prompt | llm).invoke({
        "startup_name": startup_name,
        "items": items_formatted,
        "tavily_context": tavily_context,
        "naver_context": naver_context
    })
    analysis = response.content

    # ì ìˆ˜ robust íŒŒì‹±
    score = get_analysis_score(analysis, checklist)

    state["ì„±ì¥ë¥ _ì ìˆ˜"] = score
    state["ì„±ì¥ë¥ _ë¶„ì„_ê·¼ê±°"] = analysis
    return state


def internal_judgement(state: AgentState) -> AgentState:
    if (
        state["ìƒí’ˆ_ì ìˆ˜"] < 40 or
        state["ê¸°ìˆ _ì ìˆ˜"] < 40 or
        state["ì„±ì¥ë¥ _ì ìˆ˜"] < 40
    ):
        state["ìµœì¢…_íŒë‹¨"] = "ë³´ë¥˜"
    elif (
        (state["ìƒí’ˆ_ì ìˆ˜"] + state["ê¸°ìˆ _ì ìˆ˜"] + state["ì„±ì¥ë¥ _ì ìˆ˜"]) / 3 < 60
    ):
        state["ìµœì¢…_íŒë‹¨"] = "ë³´ë¥˜"
    return state

def compare_retrieval_methods(all_docs, query):
    """BM25, Cosine Similarity, Hybrid ë°©ì‹ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ëŠ” í•¨ìˆ˜"""
    import time
    
    results = {}
    
    # 1. BM25 ê²€ìƒ‰
    print("\n[ì„±ëŠ¥ ë¹„êµ] BM25 ê²€ìƒ‰ ì‹œì‘...")
    start_time = time.time()
    bm25_retriever = BM25Retriever.from_documents(all_docs)
    bm25_retriever.k = 5
    bm25_docs = bm25_retriever.get_relevant_documents(query)
    bm25_time = time.time() - start_time
    
    results['bm25'] = {
        'docs': bm25_docs,
        'time': bm25_time,
        'count': len(bm25_docs)
    }
    print(f"BM25 ê²€ìƒ‰ ì™„ë£Œ: {bm25_time:.3f}ì´ˆ, {len(bm25_docs)}ê°œ ë¬¸ì„œ")
    
    # 2. Cosine Similarity ê²€ìƒ‰
    print("\n[ì„±ëŠ¥ ë¹„êµ] Cosine Similarity ê²€ìƒ‰ ì‹œì‘...")
    start_time = time.time()
    vector_store = Chroma.from_documents(all_docs, OpenAIEmbeddings())
    vector_retriever = vector_store.as_retriever(search_kwargs={"k": 5})
    vector_docs = vector_retriever.get_relevant_documents(query)
    vector_time = time.time() - start_time
    
    results['cosine'] = {
        'docs': vector_docs,
        'time': vector_time,
        'count': len(vector_docs)
    }
    print(f"Cosine Similarity ê²€ìƒ‰ ì™„ë£Œ: {vector_time:.3f}ì´ˆ, {len(vector_docs)}ê°œ ë¬¸ì„œ")
    
    # 3. Hybrid ê²€ìƒ‰
    print("\n[ì„±ëŠ¥ ë¹„êµ] Hybrid ê²€ìƒ‰ ì‹œì‘...")
    start_time = time.time()
    ensemble_retriever = EnsembleRetriever(
        retrievers=[bm25_retriever, vector_retriever],
        weights=[0.5, 0.5]
    )
    hybrid_docs = ensemble_retriever.get_relevant_documents(query)
    hybrid_time = time.time() - start_time
    
    results['hybrid'] = {
        'docs': hybrid_docs,
        'time': hybrid_time,
        'count': len(hybrid_docs)
    }
    print(f"Hybrid ê²€ìƒ‰ ì™„ë£Œ: {hybrid_time:.3f}ì´ˆ, {len(hybrid_docs)}ê°œ ë¬¸ì„œ")
    
    # 4. ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ” ê²€ìƒ‰ ë°©ì‹ë³„ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼")
    print("="*60)
    print(f"{'ë°©ì‹':<15} {'ì†Œìš”ì‹œê°„':<10} {'ë¬¸ì„œìˆ˜':<8} {'ìƒëŒ€ì†ë„':<10}")
    print("-"*60)
    
    fastest_time = min(bm25_time, vector_time, hybrid_time)
    
    for method, data in results.items():
        relative_speed = fastest_time / data['time']
        method_name = {
            'bm25': 'BM25',
            'cosine': 'Cosine',
            'hybrid': 'Hybrid'
        }[method]
        print(f"{method_name:<15} {data['time']:<10.3f} {data['count']:<8} {relative_speed:<10.2f}x")
    
    # 5. ë¬¸ì„œ ì¤‘ë³µë„ ë¶„ì„
    print("\nğŸ“Š ë¬¸ì„œ ì¤‘ë³µë„ ë¶„ì„:")
    all_doc_contents = []
    for method, data in results.items():
        method_docs = [doc.page_content[:100] for doc in data['docs']]
        all_doc_contents.extend(method_docs)
    
    unique_docs = len(set(all_doc_contents))
    total_docs = len(all_doc_contents)
    diversity_score = unique_docs / total_docs if total_docs > 0 else 0
    
    print(f"ì´ ê²€ìƒ‰ëœ ë¬¸ì„œ: {total_docs}ê°œ")
    print(f"ê³ ìœ  ë¬¸ì„œ: {unique_docs}ê°œ")
    print(f"ë‹¤ì–‘ì„± ì ìˆ˜: {diversity_score:.2f} (1.0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë‹¤ì–‘í•¨)")
    
    return results

def analyze_market(state: Dict[str, Any]) -> Dict[str, Any]:
    startup_name = state.get("startup_name", "")
    if not startup_name:
        state["ì‹œì¥ì„±_ì ìˆ˜"] = 0
        state["ì‹œì¥ì„±_ë¶„ì„_ê·¼ê±°"] = "ìŠ¤íƒ€íŠ¸ì—… ì´ë¦„ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        return state

    # âœ… PDF í´ë” ë‚´ ì „ì²´ PDF ë¡œë“œ
    pdf_dir = os.path.join(os.getcwd(), "data")
    pdf_files = [f for f in os.listdir(pdf_dir) if f.lower().endswith(".pdf")]
    print(f"\n[DEBUG] í´ë” ë‚´ PDF íŒŒì¼ ìˆ˜: {len(pdf_files)}")

    all_docs = []
    for pdf_file in pdf_files:
        pdf_path = os.path.join(pdf_dir, pdf_file)
        loader = PyMuPDFLoader(pdf_path)
        split_docs = loader.load_and_split(RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100))
        print(f"[DEBUG] '{pdf_file}' â†’ {len(split_docs)} ì²­í¬ ìƒì„±")
        all_docs.extend(split_docs)

    # ì„±ëŠ¥ ë¹„êµ ì‹¤í–‰ (ì„ íƒì )
    query = f"{startup_name} ì‹œì¥ì„±, ì‹œì¥ ê·œëª¨, ì„±ì¥ì„±, ìˆ˜ìš” ë™í–¥, íŠ¸ë Œë“œ"
    
    # ì„±ëŠ¥ ë¹„êµë¥¼ ì›í•˜ë©´ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”
    # comparison_results = compare_retrieval_methods(all_docs, query)
    
    # Cosine Similarityë§Œ ì‚¬ìš© (ë²¡í„° ê²€ìƒ‰)
    vector_store = Chroma.from_documents(all_docs, OpenAIEmbeddings())
    vector_retriever = vector_store.as_retriever(search_kwargs={"k": 5})
    
    # ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰
    retrieved_docs = vector_retriever.get_relevant_documents(query)
    print(f"\n[DEBUG] Cosine Similarity RAG ê²€ìƒ‰ ê²°ê³¼ - ì´ {len(retrieved_docs)}ê°œ")
    for i, doc in enumerate(retrieved_docs):
        print(f"\n[ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ {i+1}] (Page: {doc.metadata.get('page', 'ì•Œ ìˆ˜ ì—†ìŒ')})\n{doc.page_content[:300]}...")

    rag_context = "\n\n".join([f"(Page: {doc.metadata.get('page', 'ì•Œ ìˆ˜ ì—†ìŒ')})\n{doc.page_content}" for doc in retrieved_docs]) or "PDFì—ì„œ ìœ ì˜ë¯¸í•œ ì •ë³´ ì—†ìŒ"

    # âœ… Web Search part (Tavily)
    search_tool = TavilySearchResults(k=10)
    web_results = search_tool.invoke(f"{startup_name} AI ìŠ¤íƒ€íŠ¸ì—… ì‹œì¥ì„±, ì‹œì¥ ê·œëª¨, ì„±ì¥ì„±, ìˆ˜ìš” ë™í–¥, íŠ¸ë Œë“œ ìµœê·¼ 6ê°œì›” ê¸°ì‚¬")
    web_context = "\n".join([f"{i+1}. {result['title']} ({result['url']})" for i, result in enumerate(web_results)]) or "ì›¹ ê²€ìƒ‰ì—ì„œ ìœ ì˜ë¯¸í•œ ì •ë³´ ì—†ìŒ"

    combined_context = f"[PDF ê¸°ë°˜ RAG ê²€ìƒ‰ ê²°ê³¼]\n{rag_context}\n\n[ì›¹ ê²€ìƒ‰ ê²°ê³¼]\n{web_context}"

    print(f"\n[DEBUG] ìµœì¢… combined_context (ì• 1000ì):\n{combined_context[:1000]}...")

    checklist = [
        "ì‹œì¥ ê·œëª¨ ë° ì„±ì¥ì„±",
        "ì‚°ì—… ë‚´ ìˆ˜ìš” íŠ¸ë Œë“œ",
        "ê³ ê°êµ° ë‹¤ì–‘ì„± ë° í™•ë³´ ê°€ëŠ¥ì„±",
        "ì‹œì¥ ì§„ì… ê°€ëŠ¥ì„± (ê·œì œ, ì¥ë²½ ë“±)",
        "ì‹œì¥ ë‚´ ëŒ€ì²´ì¬/ê²½ìŸ ì œí’ˆ ì¡´ì¬ ì—¬ë¶€",
        "í–¥í›„ 3~5ë…„ ì„±ì¥ ì „ë§",
        "êµ­ë‚´ì™¸ ì‹œì¥ í™•ì¥ ê°€ëŠ¥ì„±",
        "ì‚°ì—… ë‚´ íŒŒíŠ¸ë„ˆì‰½ ë° ìƒíƒœê³„ ê°€ëŠ¥ì„±",
        "ì‚¬íšŒì /ê²½ì œì  ë©”ê°€íŠ¸ë Œë“œ ë¶€í•© ì—¬ë¶€",
        "ê¸°ìˆ  ë³€í™”ì— ë”°ë¥¸ ì‹œì¥ ìœ„í—˜ì„±"
    ]

    prompt = ChatPromptTemplate.from_template(
        "ë‹¹ì‹ ì€ AI ìŠ¤íƒ€íŠ¸ì—… ì‹œì¥ì„± í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. '{startup_name}'ì˜ ì‹œì¥ì„±ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•´ ì£¼ì„¸ìš”.\n\n"
        "ë‹¤ìŒ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”:\n{combined_context}\n\n"
        "ì²´í¬ë¦¬ìŠ¤íŠ¸:\n" +
        "\n".join([f"{i+1}. {q}" for i, q in enumerate(checklist)]) + "\n\n"
        "ê° í•­ëª©ì€ 0ì ì—ì„œ 10ì  ì‚¬ì´ë¡œ ììœ ë¡­ê²Œ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ì„¸ìš”.\n"
        "ì‘ë‹µ í˜•ì‹:\n"
        "- ê° í•­ëª©ë³„ ì ìˆ˜(0~10)ì™€ **ì¶œì²˜ í¬í•¨í•œ ë¶„ì„ ê·¼ê±° (ì¶œì²˜ëŠ” ê¸°ì‚¬ ì œëª©, URL ë˜ëŠ” PDF í˜ì´ì§€ ë²ˆí˜¸ ëª…ì‹œ)**\n"
        "- ê²°ë¡  ë° ì¢…í•© ë¶„ì„\n"
        "- ì´ì : ì ìˆ˜ (ìˆ«ìë§Œ ì…ë ¥í•˜ì„¸ìš”, ì˜ˆ: 75)"
    )

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    chain = prompt | llm

    response = chain.invoke({
        "startup_name": startup_name,
        "combined_context": combined_context,
    })

    analysis = response.content
    print(f"\n[DEBUG] LLM ì‘ë‹µ (ì• 1000ì):\n{analysis[:1000]}...")

    score = get_analysis_score(analysis, checklist)

    print(f"\n[DEBUG] ìµœì¢… ì´ì : {score}")
    state["ì‹œì¥ì„±_ì ìˆ˜"] = score
    state["ì‹œì¥ì„±_ë¶„ì„_ê·¼ê±°"] = analysis
    return state




def analyze_competitor(state: Dict[str, Any]) -> Dict[str, Any]:
    startup_name = state.get("startup_name", "")
    if not startup_name:
        state["ê²½ìŸì‚¬_ì ìˆ˜"] = 0
        state["ê²½ìŸì‚¬_ë¶„ì„_ê·¼ê±°"] = "ìŠ¤íƒ€íŠ¸ì—… ì´ë¦„ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        print("[DEBUG] ìŠ¤íƒ€íŠ¸ì—… ì´ë¦„ ì—†ìŒ")
        return state

    search_tool = TavilySearchResults(k=20)
    search_results = search_tool.invoke(f"{startup_name} ê²½ìŸì‚¬ AI ìŠ¤íƒ€íŠ¸ì—… ì‹œì¥ ë¶„ì„ ìµœê·¼ 6ê°œì›” ê¸°ì‚¬")

    print(f"\n[DEBUG] ê²€ìƒ‰ëœ ê¸°ì‚¬ ìˆ˜: {len(search_results)}ê°œ")
    for idx, result in enumerate(search_results, 1):
        print(f"{idx}. {result['title']}")

    if len(search_results) < 10:
        print("[ê²½ê³ ] ê²€ìƒ‰ëœ ê¸°ì‚¬ ìˆ˜ê°€ 10ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤. ì •ë³´ ë¶€ì¡±ìœ¼ë¡œ ë¶„ì„ ì‹ ë¢°ë„ê°€ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    checklist = [
        "ì‹œì¥ ì§„ì… ì¥ë²½ ë¶„ì„",
        "ì£¼ìš” ê²½ìŸì‚¬ ì‹ë³„",
        "ê²½ìŸì‚¬ ì œí’ˆ/ì„œë¹„ìŠ¤ ì°¨ë³„ì ",
        "ì‹œì¥ ì ìœ ìœ¨ ë°ì´í„°",
        "ê°€ê²© ì „ëµ ë¹„êµ",
        "ê¸°ìˆ ì  ìš°ìœ„ ë¶„ì„",
        "íƒ€ê²Ÿ ê³ ê°ì¸µ ì¤‘ë³µë„",
        "ì„±ì¥ ì†ë„ ë° ì¶”ì„¸",
        "íˆ¬ì ìœ ì¹˜ ìƒí™©",
        "ê²½ìŸì‚¬ ëŒ€ì‘ ì „ëµ ê°€ëŠ¥ì„±"
    ]

    prompt = ChatPromptTemplate.from_template(
        "ë‹¹ì‹ ì€ AI ìŠ¤íƒ€íŠ¸ì—… ê²½ìŸ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. '{startup_name}'ì˜ ê²½ìŸì‚¬ í™˜ê²½ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•´ ì£¼ì„¸ìš”.\n\n"
        "ìµœê·¼ 6ê°œì›” ì´ë‚´ì— ë°œí–‰ëœ 20ê°œ ì´ìƒì˜ ê¸°ì‚¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ë‹¤ìŒ ì²´í¬ë¦¬ìŠ¤íŠ¸ì˜ ê° í•­ëª©ì„ í‰ê°€í•˜ì„¸ìš”.\n"
        "ê° í•­ëª©ì€ 0ì ì—ì„œ 10ì  ì‚¬ì´ë¡œ ììœ ë¡­ê²Œ ì ìˆ˜ë¥¼ ë¶€ì—¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì ìˆ˜ ë¶€ì—¬ ê¸°ì¤€:\n"
        "- ë§¤ìš° ìš°ìˆ˜í•˜ê±°ë‚˜ ì¶©ë¶„íˆ ì¶©ì¡± â†’ 9~10ì \n"
        "- ì¼ë¶€ ì¶©ì¡±ë˜ì—ˆê±°ë‚˜ ë¶ˆì™„ì „ â†’ 5~8ì \n"
        "- ê±°ì˜ ì •ë³´ê°€ ì—†ê±°ë‚˜ ì¶©ì¡±ë˜ì§€ ì•ŠìŒ â†’ 0~4ì \n\n"
        "ì›¹ ê²€ìƒ‰ ê²°ê³¼:\n{search_results}\n\n"
        "ì²´í¬ë¦¬ìŠ¤íŠ¸:\n"
        "1. ì‹œì¥ ì§„ì… ì¥ë²½ ë¶„ì„\n"
        "2. ì£¼ìš” ê²½ìŸì‚¬ ì‹ë³„\n"
        "3. ê²½ìŸì‚¬ ì œí’ˆ/ì„œë¹„ìŠ¤ ì°¨ë³„ì \n"
        "4. ì‹œì¥ ì ìœ ìœ¨ ë°ì´í„°\n"
        "5. ê°€ê²© ì „ëµ ë¹„êµ\n"
        "6. ê¸°ìˆ ì  ìš°ìœ„ ë¶„ì„\n"
        "7. íƒ€ê²Ÿ ê³ ê°ì¸µ ì¤‘ë³µë„\n"
        "8. ì„±ì¥ ì†ë„ ë° ì¶”ì„¸\n"
        "9. íˆ¬ì ìœ ì¹˜ ìƒí™©\n"
        "10. ê²½ìŸì‚¬ ëŒ€ì‘ ì „ëµ ê°€ëŠ¥ì„±\n\n"
        "ì‘ë‹µ í˜•ì‹:\n"
        "- ê° í•­ëª©ë³„ ì ìˆ˜(0~10)ì™€ **ì¶œì²˜ í¬í•¨í•œ ë¶„ì„ ê·¼ê±° (ì¶œì²˜ëŠ” ê¸°ì‚¬ ì œëª© ë˜ëŠ” URL ëª…ì‹œ)**\n"
        "- ê²°ë¡  ë° ì¢…í•© ë¶„ì„\n"
        "- ì´ì : ì ìˆ˜ (ìˆ«ìë§Œ ì…ë ¥í•˜ì„¸ìš”, ì˜ˆ: 75)"
    )

    chain = prompt | llm
    response = chain.invoke({
        "startup_name": startup_name,
        "search_results": search_results
    })

    analysis = response.content
    print("\n[DEBUG] LLM ì‘ë‹µ ë‚´ìš©:")
    print(analysis)

    score = extract_total_score_from_analysis(analysis)
    if score is None:
        print("\n[DEBUG] ì´ì  íŒŒì‹± ì‹¤íŒ¨ â†’ extract_checklist_scores_competitor()ì—ì„œ í•­ëª©ë³„ ì ìˆ˜ ì§ì ‘ ê³„ì‚°")
        score = extract_checklist_scores_competitor(analysis, checklist)

    state["ê²½ìŸì‚¬_ì ìˆ˜"] = score
    state["ê²½ìŸì‚¬_ë¶„ì„_ê·¼ê±°"] = analysis
    print(f"[DEBUG] ìµœì¢… ì´ì : {score}")
    return state


def extract_total_score_from_analysis(analysis: str) -> int:
    """LLM ì‘ë‹µì—ì„œ ë‹¤ì–‘í•œ ì´ì  í‘œí˜„ì„ robustí•˜ê²Œ íŒŒì‹±"""
    patterns = [
        r"\*\*ì´ì \*\*[:ï¼š]?\s*(\d{1,3})",       # '**ì´ì **: 69'
        r"ì´ì [:ï¼š]?\s*(\d{1,3})\s*(?:ì |/100)?",  # 'ì´ì : 69', 'ì´ì : 69ì '
        r"Score[:ï¼š]?\s*(\d{1,3})\s*(?:ì |/100)?",  # 'Score: 69', 'Score: 69/100'
    ]
    for pattern in patterns:
        match = re.search(pattern, analysis, re.IGNORECASE)
        if match:
            print(f"[DEBUG] ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì§ì ‘ íŒŒì‹±ëœ ì´ì : {match.group(1)}")
            return int(match.group(1))
    return None

def extract_checklist_scores(analysis: str, checklist: List[str]) -> int:
    """ì²´í¬ë¦¬ìŠ¤íŠ¸ í•­ëª©ë³„ ì ìˆ˜ë¥¼ ìœ ì—°í•˜ê²Œ íŒŒì‹± (ë‹¤ì–‘í•œ í‘œí˜„ í—ˆìš©)"""
    # âœ… ì´ì  ì œê±°
    clean_analysis = re.sub(r"ì´ì [:ï¼š]?\s*\d{1,3}\s*(?:ì |/100)?", "", analysis, flags=re.IGNORECASE)

    total_score = 0
    print("\n[DEBUG] ì²´í¬ë¦¬ìŠ¤íŠ¸ë³„ ì ìˆ˜ íŒŒì‹± ì‹œì‘:")
    for i, item in enumerate(checklist, 1):
        # ëª¨ë“  í•­ëª© ê³µí†µì ìœ¼ë¡œ ì‚¬ìš©í•  íŒ¨í„´ ë¦¬ìŠ¤íŠ¸ (ê°€ì¥ ì¼ë°˜ì  â†’ êµ¬ì²´ì  ìˆœì„œë¡œ)
        patterns = [
            fr"{i}\.\s.*?(\d{{1,2}})\s*/\s*10",  # '9/10'
            fr"{i}\.\s.*?(\d{{1,2}})ì ",         # '9ì '
            fr"{i}\.\s.*?ì ìˆ˜[:ï¼š]?\s*(\d{{1,2}})",  # 'ì ìˆ˜: 9'
            fr"{item}.*?(\d{{1,2}})\s*/\s*10",
            fr"{item}.*?(\d{{1,2}})ì ",
            fr"{item}.*?ì ìˆ˜[:ï¼š]?\s*(\d{{1,2}})",
        ]
        found = False
        for pattern in patterns:
            match = re.search(pattern, clean_analysis, re.DOTALL | re.IGNORECASE)
            if match:
                item_score = int(match.group(1))
                print(f"- í•­ëª© {i}: {item} â†’ ì ìˆ˜: {item_score}")
                total_score += item_score
                found = True
                break
        if not found:
            print(f"- í•­ëª© {i}: {item} â†’ ì ìˆ˜ ì°¾ì§€ ëª»í•¨ (0ì  ì²˜ë¦¬)")

    print(f"[DEBUG] ì²´í¬ë¦¬ìŠ¤íŠ¸ ì´í•© (ì •í™•í•œ í•©ì‚°): {total_score}")
    return min(100, max(0, total_score))

# %%
def final_judgement(state: AgentState) -> AgentState:
    avg_internal = (state["ìƒí’ˆ_ì ìˆ˜"] + state["ê¸°ìˆ _ì ìˆ˜"] + state["ì„±ì¥ë¥ _ì ìˆ˜"]) / 3
    avg_total = (avg_internal + state["ì‹œì¥ì„±_ì ìˆ˜"] + state["ê²½ìŸì‚¬_ì ìˆ˜"]) / 3
    state["ìµœì¢…_íŒë‹¨"] = "íˆ¬ì" if avg_total >= 65 else "ë³´ë¥˜"
    return state

# %%
def generate_report(state: AgentState) -> AgentState:
    startup_name = state.get("startup_name", "ì•Œ ìˆ˜ ì—†ëŠ” ìŠ¤íƒ€íŠ¸ì—…")

    prompt = ChatPromptTemplate.from_template(
        "ìŠ¤íƒ€íŠ¸ì—… '{startup_name}'ì— ëŒ€í•œ íˆ¬ì ì‹¬ì‚¬ ê²°ê³¼ëŠ” {ìµœì¢…_íŒë‹¨} ì…ë‹ˆë‹¤. ì ìˆ˜ ìš”ì•½:\n"
        "- ìƒí’ˆ/ì„œë¹„ìŠ¤: {ìƒí’ˆ_ì ìˆ˜}\n"
        "- ê¸°ìˆ : {ê¸°ìˆ _ì ìˆ˜}\n"
        "- ì„±ì¥ë¥ : {ì„±ì¥ë¥ _ì ìˆ˜}\n"
        "- ì‹œì¥ì„±: {ì‹œì¥ì„±_ì ìˆ˜}\n"
        "- ê²½ìŸì‚¬: {ê²½ìŸì‚¬_ì ìˆ˜}\n\n"
        "ê° í•­ëª©ë³„ ë¶„ì„ ê·¼ê±°:\n"
        "1. ìƒí’ˆ/ì„œë¹„ìŠ¤ ë¶„ì„:\n{ìƒí’ˆ_ë¶„ì„_ê·¼ê±°}\n\n"
        "2. ê¸°ìˆ  ë¶„ì„:\n{ê¸°ìˆ _ë¶„ì„_ê·¼ê±°}\n\n"
        "3. ì„±ì¥ë¥  ë¶„ì„:\n{ì„±ì¥ë¥ _ë¶„ì„_ê·¼ê±°}\n\n"
        "4. ì‹œì¥ì„± ë¶„ì„:\n{ì‹œì¥ì„±_ë¶„ì„_ê·¼ê±°}\n\n"
        "5. ê²½ìŸì‚¬ ë¶„ì„:\n{ê²½ìŸì‚¬_ë¶„ì„_ê·¼ê±°}\n\n"
        "ìœ„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ íˆ¬ì ì‹¬ì‚¬ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”. ê° í•­ëª©ë³„ ê°•ì ê³¼ ì•½ì ì„ ìš”ì•½í•˜ê³ , "
        "ìµœì¢… íŒë‹¨ì˜ ê·¼ê±°ë¥¼ ëª…í™•íˆ ì œì‹œí•˜ë©°, ê°œì„ ì´ í•„ìš”í•œ ë¶€ë¶„ì— ëŒ€í•œ ì œì•ˆë„ í¬í•¨í•´ì£¼ì„¸ìš”."
    )
    chain = prompt | llm
    report = chain.invoke({
        "startup_name": startup_name,
        "ìµœì¢…_íŒë‹¨": state.get("ìµœì¢…_íŒë‹¨", "ë³´ë¥˜"),
        "ìƒí’ˆ_ì ìˆ˜": state.get("ìƒí’ˆ_ì ìˆ˜", 0),
        "ê¸°ìˆ _ì ìˆ˜": state.get("ê¸°ìˆ _ì ìˆ˜", 0),
        "ì„±ì¥ë¥ _ì ìˆ˜": state.get("ì„±ì¥ë¥ _ì ìˆ˜", 0),
        "ì‹œì¥ì„±_ì ìˆ˜": state.get("ì‹œì¥ì„±_ì ìˆ˜", 0),
        "ê²½ìŸì‚¬_ì ìˆ˜": state.get("ê²½ìŸì‚¬_ì ìˆ˜", 0),
        "ìƒí’ˆ_ë¶„ì„_ê·¼ê±°": state.get("ìƒí’ˆ_ë¶„ì„_ê·¼ê±°", "ì •ë³´ ì—†ìŒ"),
        "ê¸°ìˆ _ë¶„ì„_ê·¼ê±°": state.get("ê¸°ìˆ _ë¶„ì„_ê·¼ê±°", "ì •ë³´ ì—†ìŒ"),
        "ì„±ì¥ë¥ _ë¶„ì„_ê·¼ê±°": state.get("ì„±ì¥ë¥ _ë¶„ì„_ê·¼ê±°", "ì •ë³´ ì—†ìŒ"),
        "ì‹œì¥ì„±_ë¶„ì„_ê·¼ê±°": state.get("ì‹œì¥ì„±_ë¶„ì„_ê·¼ê±°", "ì •ë³´ ì—†ìŒ"),
        "ê²½ìŸì‚¬_ë¶„ì„_ê·¼ê±°": state.get("ê²½ìŸì‚¬_ë¶„ì„_ê·¼ê±°", "ì •ë³´ ì—†ìŒ")
    })
    state["ë³´ê³ ì„œ"] = report.content
    return state


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í™˜ê²½ë³„ ê²½ë¡œ ìˆ˜ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_DIR = os.getcwd()

WKHTMLTOPDF_BIN = os.path.join(BASE_DIR, "wkhtmltopdf", "bin", "wkhtmltopdf.exe")
NANUM_REG_TTF   = os.path.join(BASE_DIR, "font", "NanumGothic.ttf")
NANUM_BOLD_TTF  = os.path.join(BASE_DIR, "font", "NanumGothicBold.ttf")
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# í•­ìƒ ë“¤ì–´ê°ˆ ì•ˆë‚´ ë¬¸êµ¬
NOTICE = "ìƒí’ˆ/ì„œë¹„ìŠ¤, ê¸°ìˆ , ì„±ì¥ë¥  ì¤‘ì— í•˜ë‚˜ë¼ë„ 40ì  ë¯¸ë§Œì´ê±°ë‚˜ í‰ê·  60ì  ë¯¸ë§Œì´ë©´ ì‹œì¥ì„±ê³¼ ê²½ìŸì‚¬ í•­ëª©ì€ ì¸¡ì •ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤."


# 1) Markdown ìƒì„±
def generate_markdown(state: dict, md_path: str) -> None:
    sn  = state.get("startup_name", "ì•Œ ìˆ˜ ì—†ëŠ” ìŠ¤íƒ€íŠ¸ì—…")
    rep = state.get("ë³´ê³ ì„œ",       "ë³´ê³ ì„œ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
    dec = state.get("ìµœì¢…_íŒë‹¨",    "ë³´ë¥˜")

    labels = ["ìƒí’ˆ/ì„œë¹„ìŠ¤","ê¸°ìˆ ","ì„±ì¥ë¥ ","ì‹œì¥ì„±","ê²½ìŸì‚¬"]
    keys   = ["ìƒí’ˆ_ì ìˆ˜","ê¸°ìˆ _ì ìˆ˜","ì„±ì¥ë¥ _ì ìˆ˜","ì‹œì¥ì„±_ì ìˆ˜","ê²½ìŸì‚¬_ì ìˆ˜"]
    scores = [int(state.get(k,0)) for k in keys]
    avg    = sum(scores)/len(scores)

    dec_html = ("<span style='color:green;'>íˆ¬ì</span>"
                if dec=="íˆ¬ì" else "<span style='color:red;'>ë³´ë¥˜</span>")

    md = [
        f"<h1 align='center'>{sn} íˆ¬ì ë¶„ì„ ë³´ê³ ì„œ</h1>", "",
        f"- **ì‘ì„±ì¼** : {datetime.datetime.now():%Yë…„ %mì›” %dì¼}",
        f"- **ìµœì¢… íŒë‹¨** : {dec_html}", "", "---", "",
        "## 1. ì ìˆ˜ ìš”ì•½", "",
        "| í‰ê°€ í•­ëª© | ì ìˆ˜ |", "|:-----------:|:----:|",
        *[f"| {l} | **{v}** |" for l,v in zip(labels,scores)],
        f"| **í‰ê· ** | **{avg:.1f}** |", "",
        f"> **{NOTICE}**",                    # ì•ˆë‚´ë¬¸ ì‚½ì…
        "", "---", "",
        "## 2. ìƒì„¸ ë¶„ì„", ""
    ]

    # ìƒì„¸ ë¶„ì„ â€“ ì¤„ë°”ê¿ˆ ë¡œì§ ê·¸ëŒ€ë¡œ ìœ ì§€
    for line in rep.strip().splitlines():
        if   line.startswith("### "): md.append(f"#### **{line[4:]}**")
        elif line.startswith("## " ): md.append(f"### **{line[3:]}**")
        elif line.startswith("# "  ): md.append(f"## **{line[2:]}**")
        else:                         md.append(line)

    md += ["", "---", "*ì´ ë³´ê³ ì„œëŠ” AI ë¶„ì„ ì‹œìŠ¤í…œì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*"]

    with open(md_path, "w", encoding="utf-8") as f:
        f.write("\n".join(md))


# 2) Markdown â†’ PDF (wkhtmltopdf)
def convert_md_to_pdf(md_path:str, pdf_path:str) -> None:
    cfg = pdfkit.configuration(wkhtmltopdf=WKHTMLTOPDF_BIN)
    with open(md_path,"r",encoding="utf-8") as f:
        body_html = markdown2.markdown(f.read(), extras=["tables"])

    if os.path.isfile(NANUM_REG_TTF):
        reg  = os.path.abspath(NANUM_REG_TTF).replace("\\","/")
        bold = os.path.abspath(NANUM_BOLD_TTF if os.path.isfile(NANUM_BOLD_TTF) else NANUM_REG_TTF).replace("\\","/")
        font_css = (f"@font-face{{font-family:'NanumGothic';src:url('file:///{reg}') format('truetype');font-weight:normal;}}"
                    f"@font-face{{font-family:'NanumGothic';src:url('file:///{bold}') format('truetype');font-weight:bold;}}")
        family = "NanumGothic"
    else:
        font_css, family = "", "sans-serif"

    style = f"""
    <style>
    {font_css}
    body{{margin:40px 50px 60px;font-family:'{family}';line-height:1.6;font-size:11pt}}
    h1{{font-size:20pt;text-align:center;margin-bottom:0.6em}}
    h2{{font-size:15pt;margin-top:1.5em;margin-bottom:0.4em}}
    table{{border-collapse:collapse;width:100%;margin-top:0.8em;font-size:10.5pt}}
    th,td{{border:1px solid #666;padding:6px 8px;text-align:center}}
    th{{background:#e0e0e0;font-weight:bold}}
    tr:last-child td{{background:#f5f5f5;font-weight:bold}}
    </style>"""

    html = f"<!DOCTYPE html><html><head><meta charset='utf-8'>{style}</head><body>{body_html}</body></html>"
    pdfkit.from_string(html, pdf_path, configuration=cfg,
                       options={"enable-local-file-access":None,"encoding":"utf-8"})


# 3) ReportLab PDF + MarkdownPDF í†µí•©
def generate_pdf(state: dict) -> dict:
    sn     = state.get("startup_name","ì•Œ ìˆ˜ ì—†ëŠ” ìŠ¤íƒ€íŠ¸ì—…")
    today  = datetime.datetime.now().strftime("%Y%m%d")
    out    = "investment_reports"; os.makedirs(out, exist_ok=True)
    lab_pdf = os.path.join(out, f"{sn}_íˆ¬ìë¶„ì„ë³´ê³ ì„œ_{today}_lab.pdf")
    md_path = os.path.join(out, f"{sn}_ë³´ê³ ì„œ.md")
    htmlpdf = os.path.join(out, f"{sn}_íˆ¬ìë¶„ì„ë³´ê³ ì„œ_{today}.pdf")

    doc, styles = SimpleDocTemplate(lab_pdf, pagesize=letter), getSampleStyleSheet()
    try:
        pdfmetrics.registerFont(TTFont("NanumGothic",      NANUM_REG_TTF))
        pdfmetrics.registerFont(TTFont("NanumGothic-Bold", NANUM_BOLD_TTF))
        base, bold = "NanumGothic", "NanumGothic-Bold"
    except Exception:
        base, bold = "Helvetica", "Helvetica-Bold"

    styles.add(ParagraphStyle("ReportTitle",   parent=styles["Heading1"], fontSize=18, alignment=1, spaceAfter=20, fontName=base))
    styles.add(ParagraphStyle("ReportSubtitle",parent=styles["Heading2"], fontSize=14, spaceBefore=10, spaceAfter=10, fontName=base))
    styles.add(ParagraphStyle("SectionTitle",  parent=styles["Normal"],   fontSize=12, spaceBefore=8, spaceAfter=6, fontName=bold))
    for n in ["Normal","Italic","Heading1","Heading2","Heading3","Heading4","Heading5","Heading6"]:
        styles[n].fontName = base

    elems = [
        Paragraph(f"{sn} íˆ¬ì ë¶„ì„ ë³´ê³ ì„œ", styles["ReportTitle"]),
        Paragraph(f"ì‘ì„±ì¼: {datetime.datetime.now():%Yë…„ %mì›” %dì¼}", styles["Normal"]),
        Spacer(1,20)
    ]

    color = "green" if state.get("ìµœì¢…_íŒë‹¨")=="íˆ¬ì" else "red"
    elems += [Paragraph(f"ìµœì¢… íŒë‹¨: <font color='{color}'><b>{state.get('ìµœì¢…_íŒë‹¨','ë³´ë¥˜')}</b></font>",
                        styles["ReportSubtitle"]), Spacer(1,10)]

    keys   = ["ìƒí’ˆ_ì ìˆ˜","ê¸°ìˆ _ì ìˆ˜","ì„±ì¥ë¥ _ì ìˆ˜","ì‹œì¥ì„±_ì ìˆ˜","ê²½ìŸì‚¬_ì ìˆ˜"]
    labels = ["ìƒí’ˆ/ì„œë¹„ìŠ¤","ê¸°ìˆ ","ì„±ì¥ë¥ ","ì‹œì¥ì„±","ê²½ìŸì‚¬"]
    scores = [int(state.get(k,0)) for k in keys]
    avg    = sum(scores)/len(scores)

    t_data = [["í‰ê°€ í•­ëª©","ì ìˆ˜"]] + [[l,str(s)] for l,s in zip(labels,scores)] + [["í‰ê· ",f"{avg:.1f}"]]
    t = Table(t_data, colWidths=[300,100])
    t.setStyle(TableStyle([
        ("BACKGROUND",(0,0),(1,0),colors.grey), ("TEXTCOLOR",(0,0),(1,0),colors.whitesmoke),
        ("ALIGN",(0,0),(1,0),"CENTER"), ("FONTNAME",(0,0),(1,0),bold),
        ("FONTNAME",(0,1),(-1,-1),base), ("GRID",(0,0),(-1,-1),1,colors.black),
        ("BACKGROUND",(0,-1),(1,-1),colors.lightgrey)
    ]))
    elems += [
        Paragraph("ì ìˆ˜ ìš”ì•½", styles["ReportSubtitle"]),
        t, Spacer(1,12),
        Paragraph(NOTICE, styles["Normal"]),    # ì•ˆë‚´ë¬¸ ì‚½ì…
        Spacer(1,20)
    ]

    elems.append(Paragraph("ìƒì„¸ ë¶„ì„", styles["ReportSubtitle"]))
    for ln in state.get("ë³´ê³ ì„œ","").splitlines():
        if   ln.startswith("### "): elems.append(Paragraph(ln[4:], styles["SectionTitle"]))
        elif ln.startswith("## " ): elems.append(Paragraph(ln[3:], styles["SectionTitle"]))
        elif ln.startswith("# "  ): elems.append(Paragraph(ln[2:], styles["SectionTitle"]))
        elif ln.strip():           elems.append(Paragraph(ln.strip(), styles["Normal"]))
        else:                      elems.append(Spacer(1,6))

    elems += [Spacer(1,30),
              Paragraph("ì´ ë³´ê³ ì„œëŠ” AI ë¶„ì„ ì‹œìŠ¤í…œì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. "
                        f"Â© {datetime.datetime.now().year}", styles["Italic"])]
    doc.build(elems)

    # Markdown + wkhtmltopdf
    generate_markdown(state, md_path)
    convert_md_to_pdf(md_path, htmlpdf)

    state.update(pdf_path_reportlab = lab_pdf,
                 pdf_path_wkhtml   = htmlpdf,
                 markdown_path     = md_path,
                 pdf_path          = htmlpdf)
    return state

# %%
# 4. Graph ì •ì˜ ë° ì—°ê²°
graph = StateGraph(AgentState)

graph.add_node("AnalyzeProduct", analyze_product)
graph.add_node("AnalyzeTechnology", analyze_technology)
graph.add_node("AnalyzeGrowth", analyze_growth)
graph.add_node("InternalJudgement", internal_judgement)
graph.add_node("AnalyzeMarket", analyze_market)
graph.add_node("AnalyzeCompetitor", analyze_competitor)
graph.add_node("FinalJudgement", final_judgement)
graph.add_node("GenerateReport", generate_report)
graph.add_node("GeneratePDF", generate_pdf)  # PDF ìƒì„± ë…¸ë“œ ì¶”ê°€

graph.set_entry_point("AnalyzeProduct")

graph.add_edge("AnalyzeProduct", "AnalyzeTechnology")
graph.add_edge("AnalyzeTechnology", "AnalyzeGrowth")
graph.add_edge("AnalyzeGrowth", "InternalJudgement")

# ì •ì˜í•œ ì¡°ê±´ì— ë”°ë¼ ë‹¤ë¥¸ ë…¸ë“œë¡œ ì´ë™
def route_after_internal_judgement(state: AgentState) -> str:
    if state.get("ìµœì¢…_íŒë‹¨") == "ë³´ë¥˜":
        return "GenerateReport"
    return "AnalyzeMarket"

graph.add_conditional_edges(
    "InternalJudgement",
    route_after_internal_judgement
)

graph.add_edge("AnalyzeMarket", "AnalyzeCompetitor")
graph.add_edge("AnalyzeCompetitor", "FinalJudgement")
graph.add_edge("FinalJudgement", "GenerateReport")
graph.add_edge("GenerateReport", "GeneratePDF")  # ë³´ê³ ì„œ ìƒì„± í›„ PDF ìƒì„±
graph.add_edge("GeneratePDF", END)

# %%
# 5. ì„±ëŠ¥ ë¹„êµ ì „ìš© í•¨ìˆ˜
def run_performance_comparison():
    """ê²€ìƒ‰ ë°©ì‹ë“¤ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ëŠ” ì „ìš© í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸ” ê²€ìƒ‰ ë°©ì‹ ì„±ëŠ¥ ë¹„êµ ì‹œìŠ¤í…œ")
    print("=" * 60)
    
    # í…ŒìŠ¤íŠ¸ìš© ì¿¼ë¦¬
    test_queries = [
        "AI ê¸°ìˆ  ì‹œì¥ ë™í–¥",
        "ìŠ¤íƒ€íŠ¸ì—… íˆ¬ì íŠ¸ë Œë“œ",
        "ì¸ê³µì§€ëŠ¥ ì‚°ì—… ì„±ì¥ì„±"
    ]
    
    # PDF ë¬¸ì„œ ë¡œë“œ
    pdf_dir = os.path.join(os.getcwd(), "data")
    pdf_files = [f for f in os.listdir(pdf_dir) if f.lower().endswith(".pdf")]
    print(f"ğŸ“„ ë¡œë“œëœ PDF íŒŒì¼: {len(pdf_files)}ê°œ")
    
    all_docs = []
    for pdf_file in pdf_files:
        pdf_path = os.path.join(pdf_dir, pdf_file)
        loader = PyMuPDFLoader(pdf_path)
        split_docs = loader.load_and_split(RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100))
        all_docs.extend(split_docs)
    
    print(f"ğŸ“ ì´ ë¬¸ì„œ ì²­í¬: {len(all_docs)}ê°œ")
    
    # ê° ì¿¼ë¦¬ë³„ë¡œ ì„±ëŠ¥ ë¹„êµ
    for i, query in enumerate(test_queries, 1):
        print(f"\n{'='*20} ì¿¼ë¦¬ {i}: {query} {'='*20}")
        comparison_results = compare_retrieval_methods(all_docs, query)
        
        # ê²°ê³¼ ì €ì¥ (ì„ íƒì )
        # ì—¬ê¸°ì— ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ì½”ë“œë¥¼ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤

def run_query_comparison():
    print("=" * 60)
    print("ğŸ” ê²€ìƒ‰ ë°©ì‹ ì„±ëŠ¥ ë¹„êµ (ìŠ¤íƒ€íŠ¸ì—… ì´ë¦„ ê¸°ë°˜)")
    print("=" * 60)
    startup_name = input("ë¹„êµí•  ìŠ¤íƒ€íŠ¸ì—… ì´ë¦„(ì§ˆì˜)ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    if not startup_name:
        print("âŒ ìŠ¤íƒ€íŠ¸ì—… ì´ë¦„ì„ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")
        return

    # PDF ë¬¸ì„œ ë¡œë“œ
    pdf_dir = os.path.join(os.getcwd(), "data")
    pdf_files = [f for f in os.listdir(pdf_dir) if f.lower().endswith(".pdf")]
    all_docs = []
    for pdf_file in pdf_files:
        pdf_path = os.path.join(pdf_dir, pdf_file)
        loader = PyMuPDFLoader(pdf_path)
        split_docs = loader.load_and_split(RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100))
        all_docs.extend(split_docs)

    query = f"{startup_name} ì‹œì¥ì„±, ì‹œì¥ ê·œëª¨, ì„±ì¥ì„±, ìˆ˜ìš” ë™í–¥, íŠ¸ë Œë“œ"
    results = compare_retrieval_methods(all_docs, query)

    # ê° ë°©ì‹ë³„ ê²€ìƒ‰ ë¬¸ì„œ ë‚´ìš© ì¶œë ¥
    for method, data in results.items():
        print(f"\n{'='*20} {method.upper()} ê²€ìƒ‰ ê²°ê³¼ {'='*20}")
        for i, doc in enumerate(data['docs'], 1):
            print(f"[{i}] (Page: {doc.metadata.get('page', 'ì•Œ ìˆ˜ ì—†ìŒ')})\n{doc.page_content[:300]}...\n")

# %%
# 6. ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
def get_startup_name():
    """ì‚¬ìš©ìë¡œë¶€í„° ë¶„ì„í•  AI ìŠ¤íƒ€íŠ¸ì—… ì´ë¦„ì„ ì…ë ¥ë°›ëŠ” í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸ¤– AI ìŠ¤íƒ€íŠ¸ì—… íˆ¬ì í‰ê°€ ì‹œìŠ¤í…œ")
    print("=" * 60)
    print("ë¶„ì„í•  AI ìŠ¤íƒ€íŠ¸ì—…ì˜ ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    print("ì˜ˆì‹œ: ì—…ìŠ¤í…Œì´ì§€, ë² ìŠ¬AI, íŠ¸ì›°ë¸Œë©ìŠ¤, í´ë¡œë°”, ë„¤ì´ë²„í´ë¡œë°” ë“±")
    print("-" * 60)
    
    while True:
        startup_name = input("ìŠ¤íƒ€íŠ¸ì—… ì´ë¦„: ").strip()
        if startup_name:
            print(f"\nâœ… '{startup_name}' ìŠ¤íƒ€íŠ¸ì—… ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            print("ë¶„ì„ì—ëŠ” ëª‡ ë¶„ ì •ë„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            print("-" * 60)
            return startup_name
        else:
            print("âŒ ìŠ¤íƒ€íŠ¸ì—… ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

# %%
# 7. ì‹¤í–‰
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ¤– AI ìŠ¤íƒ€íŠ¸ì—… íˆ¬ì í‰ê°€ ì‹œìŠ¤í…œ")
    print("=" * 60)
    print("1. ìŠ¤íƒ€íŠ¸ì—… ë¶„ì„ ì‹¤í–‰")
    print("2. ê²€ìƒ‰ ë°©ì‹ ì„±ëŠ¥ ë¹„êµ (ìƒ˜í”Œ ì¿¼ë¦¬)")
    print("3. ê²€ìƒ‰ ë°©ì‹ ì„±ëŠ¥ ë¹„êµ (ìŠ¤íƒ€íŠ¸ì—… ì´ë¦„ ì…ë ¥)")
    print("-" * 60)
    
    choice = input("ì„ íƒí•˜ì„¸ìš” (1, 2, 3): ").strip()
    
    if choice == "2":
        run_performance_comparison()
    elif choice == "3":
        run_query_comparison()
    else:
        startup_name = get_startup_name()
        # Graph ì»´íŒŒì¼ ë° ì‹¤í–‰
        compiled_graph = graph.compile()
        initial_state = {"startup_name": startup_name}
        try:
            print("ğŸ”„ AI ë¶„ì„ ì‹œìŠ¤í…œì´ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤...")
            result = compiled_graph.invoke(initial_state)
            # ê²°ê³¼ ì¶œë ¥
            print("\n" + "=" * 60)
            print("âœ… ë¶„ì„ ì™„ë£Œ!")
            print("=" * 60)
            print(f"ğŸ“„ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {result['pdf_path']}")
            print(f"ğŸ“Š ì´ì : {sum([result.get('ìƒí’ˆ_ì ìˆ˜', 0), result.get('ê¸°ìˆ _ì ìˆ˜', 0), result.get('ì„±ì¥ë¥ _ì ìˆ˜', 0), result.get('ì‹œì¥ì„±_ì ìˆ˜', 0), result.get('ê²½ìŸì‚¬_ì ìˆ˜', 0)]) / 5:.1f}/100")
            print(f"ğŸ¯ ìµœì¢… íŒë‹¨: {result.get('ìµœì¢…_íŒë‹¨', 'N/A')}")
            print("\n--- ë³´ê³ ì„œ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° ---")
            print(result["ë³´ê³ ì„œ"][:500] + "...")
        except Exception as e:
            print(f"\nâŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            print("API í‚¤ ì„¤ì •ì„ í™•ì¸í•˜ê±°ë‚˜ ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

# %%
# ê°œë°œìš©: ì§ì ‘ ì‹¤í–‰ ì‹œ ê²°ê³¼ í™•ì¸
# result["ë³´ê³ ì„œ"]


